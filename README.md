# ğŸ§  AI Inference Optimization Project

This project demonstrates performance optimization for AI models by applying dynamic quantization. It focuses on *text-based medical diagnosis* and *image-based chest X-ray analysis* using pretrained models from Hugging Face. The goal is to measure and compare inference performanceâ€”*latency, **memory usage, and **throughput*â€”before and after quantization.

---

## ğŸ” Project Objectives

- Use pretrained models from Hugging Face for both *text* and *image* classification tasks.
- Apply *dynamic quantization* (torch.quantization.quantize_dynamic) to optimize models.
- Compare the original and quantized models in terms of:
  - *Latency* (inference time)
  - *Memory usage*
  - *Throughput*

---

## ğŸ§° Tech Stack

- ğŸ Python
- ğŸ¤— Hugging Face Transformers
- ğŸ§  PyTorch
- ğŸ“¦ TorchVision 
- ğŸ“ˆ plotly / Streamlit 
- ğŸ–¼ PIL
- ğŸ§ª psutil (for memory profiling)
- â± time (for latency measurement)

---

## ğŸ“¦ Pretrained Models Used

### ğŸ”¤ Text-based Diagnosis
- *Model*: bert-base-uncased
 - *Task*: Medical symptom to diagnosis classification

### ğŸ–¼ Image-based Diagnosis
- *Model*: ViT pretrained from torchvision
- *Task*: Chest X-ray image classification

---

## âš™ Installation

```bash
git clone https://github.com/CAAbishek/NavigateLabs.git
cd ai-inference-optimization


# Install dependencies
pip install -r requirement.txt
